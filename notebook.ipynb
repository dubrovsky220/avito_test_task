{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a212b0bf",
   "metadata": {},
   "source": [
    "# Тестовое задание Авито"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52f12714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "from typing import List, Tuple, Dict\n",
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d3019b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 99\n",
    "\n",
    "random.seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "torch.cuda.manual_seed(RANDOM_STATE)\n",
    "torch.cuda.manual_seed_all(RANDOM_STATE)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165b8c57",
   "metadata": {},
   "source": [
    "## Создание датасета"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d671db10",
   "metadata": {},
   "source": [
    "Код ниже выполняет следующие действия:\n",
    "1. Загружает строки из файла ./dataset_raw/articles.txt.\n",
    "2. Очищает каждую строку (Unicode-нормализация, удаление control chars, удаление эмодзи, нормализация пробелов).\n",
    "3. Разбивает на короткие сегменты (2–8 слов).\n",
    "4. Для каждого сегмента строит:\n",
    "    - вход (строка без пробелов),\n",
    "    - список меток (1 после символа, если там был пробел).\n",
    "5. Собирает словарь char2id (кириллица, латиница, цифры, знаки).\n",
    "6. Возвращает готовый датасет: X (индексы символов), y (метки), char2id.\n",
    "\n",
    "\n",
    "Для обучения модели используется файл articles.txt, взятый из датасета Complex Russian Dataset (https://www.kaggle.com/datasets/artalmaz31/complex-russian-dataset). Данный файл содержит статьи на разные темы, опубликованные на dzen.ru. Для обучения модели был взят именно этот набор текстов, так как он содержит тексты на русском языке с иногда встречающимися английскими названиями и числами, что по структуре напоминает тестовые данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8a9402d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(path: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Загружает корпус текстов из файла.\n",
    "    \n",
    "    :param path: путь к файлу (каждая строка = статья или абзац).\n",
    "    :return: список строк\n",
    "    \"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = [line.strip() for line in f if line.strip()]\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "463970eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Очищает текст:\n",
    "    - нормализует Unicode (NFC)\n",
    "    - удаляет управляющие символы\n",
    "    - нормализует пробелы\n",
    "    - удаляет эмодзи\n",
    "    \n",
    "    :param text: исходная строка\n",
    "    :return: очищенный текст\n",
    "    \"\"\"\n",
    "    # Unicode нормализация\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "    \n",
    "    # Убираем control chars\n",
    "    text = \"\".join(ch for ch in text if unicodedata.category(ch)[0] != \"C\")\n",
    "    \n",
    "    # Убираем эмодзи (символы за пределами Basic Multilingual Plane и спец.категории)\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # смайлы\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # пиктограммы\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # транспорт\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # флаги\n",
    "        u\"\\U00002700-\\U000027BF\"  # символы-стрелки\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(\"\", text)\n",
    "    \n",
    "    # Нормализация пробелов\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0cf03fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_segments(text: str, min_words: int = 2, max_words: int = 8) -> List[str]:\n",
    "    \"\"\"\n",
    "    Разбивает текст на короткие сегменты (min_words-max_words слов).\n",
    "    \n",
    "    :param text: очищенный текст\n",
    "    :param min_words: минимум слов в сегменте\n",
    "    :param max_words: максимум слов в сегменте\n",
    "    :return: список коротких сегментов\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    segments = []\n",
    "    \n",
    "    if len(words) < min_words:\n",
    "        return []\n",
    "    \n",
    "    # Берем окна разных длин\n",
    "    for start in range(len(words)):\n",
    "        for length in range(min_words, max_words + 1):\n",
    "            end = start + length\n",
    "            if end <= len(words):\n",
    "                segment = \" \".join(words[start:end])\n",
    "                segments.append(segment)\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6a37ff81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_no_space_input(segment: str) -> Tuple[str, List[int]]:\n",
    "    \"\"\"\n",
    "    Генерирует вход без пробелов и метки (0/1 после каждого символа).\n",
    "    \n",
    "    :param segment: строка с пробелами\n",
    "    :return: (вход без пробелов, список меток)\n",
    "    \"\"\"\n",
    "    input_text = segment.replace(\" \", \"\")\n",
    "    labels = []\n",
    "    i = 0\n",
    "    for word in segment.split():\n",
    "        for j, ch in enumerate(word):\n",
    "            # Если символ последний в слове, ставим 1, иначе 0\n",
    "            if j == len(word) - 1 and i < len(input_text):\n",
    "                labels.append(1)\n",
    "            else:\n",
    "                labels.append(0)\n",
    "            i += 1\n",
    "    # Последний символ не должен иметь пробела после себя\n",
    "    if labels:\n",
    "        labels[-1] = 0\n",
    "    return input_text, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "58f45d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_char2id(texts: List[str]) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Строит словарь символов для корпуса.\n",
    "    \n",
    "    :param texts: список строк (inputs без пробелов)\n",
    "    :return: словарь {символ: id}\n",
    "    \"\"\"\n",
    "    chars = set(\"\".join(texts))\n",
    "    sorted_chars = sorted(chars)\n",
    "    char2id = {ch: idx + 1 for idx, ch in enumerate(sorted_chars)}  # Нумерация с 1\n",
    "    char2id[\"<UNK>\"] = len(char2id) + 1  # Для неизвестных символов\n",
    "    return char2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f96b2edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(path: str, max_segments: int = 10000) -> Tuple[List[List[int]], List[List[int]], Dict[str, int]]:\n",
    "    \"\"\"\n",
    "    Основная функция: строит датасет из корпуса статей.\n",
    "    \n",
    "    :param path: путь к файлу с текстами\n",
    "    :param max_segments: максимум сегментов в датасете\n",
    "    :return: (X, y, char2id)\n",
    "        X — список последовательностей индексов символов (inputs)\n",
    "        y — список меток (0/1 для каждого символа)\n",
    "        char2id — словарь символов\n",
    "    \"\"\"\n",
    "    lines = read_corpus(path)\n",
    "    \n",
    "    # Очистка + сегментация\n",
    "    all_segments = []\n",
    "    for line in lines:\n",
    "        cleaned = clean_text(line)\n",
    "        segs = split_into_segments(cleaned)\n",
    "        all_segments.extend(segs)\n",
    "    \n",
    "    # Перемешиваем и берем первые max_segments\n",
    "    random.shuffle(all_segments)\n",
    "    all_segments = all_segments[:max_segments]\n",
    "    \n",
    "    # Генерация данных\n",
    "    inputs, labels = [], []\n",
    "    for seg in all_segments:\n",
    "        inp, lab = generate_no_space_input(seg)\n",
    "        if inp and lab:\n",
    "            inputs.append(inp)\n",
    "            labels.append(lab)\n",
    "    \n",
    "    # Строим словарь\n",
    "    char2id = build_char2id(inputs)\n",
    "    \n",
    "    # Преобразуем inputs в индексы\n",
    "    X = []\n",
    "    for text in inputs:\n",
    "        seq = [char2id.get(ch, char2id[\"<UNK>\"]) for ch in text]\n",
    "        X.append(seq)\n",
    "    \n",
    "    return X, labels, char2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9e9bd961",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(X, y, char2id, out_dir=\"dataset\"):\n",
    "    \"\"\"\n",
    "    Сохраняет датасет и словарь для будущего использования.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): входные данные (последовательности id символов)\n",
    "        y (np.ndarray): целевые метки (бинарные маски для пробелов)\n",
    "        char2id (dict): словарь символ → id\n",
    "        out_dir (str): путь к директории для сохранения\n",
    "    \"\"\"\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Сохраняем X и y\n",
    "    np.save(out_dir / \"X.npy\", np.array(X, dtype=object))\n",
    "    np.save(out_dir / \"y.npy\", np.array(y, dtype=object))\n",
    "\n",
    "    # Сохраняем словарь\n",
    "    with open(out_dir / \"char2id.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(char2id, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Датасет сохранён в {out_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83499fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(out_dir=\"dataset\"):\n",
    "    \"\"\"\n",
    "    Загружает ранее сохранённый датасет и словарь.\n",
    "\n",
    "    Args:\n",
    "        out_dir (str): путь к директории\n",
    "\n",
    "    Returns:\n",
    "        X (np.ndarray), y (np.ndarray), char2id (dict)\n",
    "    \"\"\"\n",
    "    out_dir = Path(out_dir)\n",
    "\n",
    "    X = np.load(out_dir / \"X.npy\", allow_pickle=True)\n",
    "    y = np.load(out_dir / \"y.npy\", allow_pickle=True)\n",
    "\n",
    "    with open(out_dir / \"char2id.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        char2id = json.load(f)\n",
    "\n",
    "    print(f\"Датасет загружен из {out_dir}\")\n",
    "    return X, y, char2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6f0406a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"./dataset_raw/articles.txt\"  # файл со статьями\n",
    "X, y, char2id = build_dataset(dataset_path, max_segments=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "924cd1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Датасет сохранён в dataset_processed\n"
     ]
    }
   ],
   "source": [
    "save_dataset(X, y, char2id, out_dir=\"./dataset_processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e58030b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Датасет загружен из dataset_processed\n"
     ]
    }
   ],
   "source": [
    "X, y, char2id = load_dataset(\"./dataset_processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bf849ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пример входа (индексы): [146, 135, 131, 129, 134, 128, 136, 140, 117, 122, 128, 136, 140, 141, 122, 131, 118, 131, 126, 135, 125, 134, 145, 135, 131, 128, 145, 127, 131, 127, 131, 130, 134, 125, 128, 122, 133, 131, 129, 125]\n",
      "Пример меток: [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "Пример сегмента: этомслучаелучшеобойтисьтолькоконсилероми\n",
      "Размер словаря: 160\n"
     ]
    }
   ],
   "source": [
    "print(\"Пример входа (индексы):\", X[0])\n",
    "print(\"Пример меток:\", y[0])\n",
    "print(\"Пример сегмента:\", \"\".join([list(char2id.keys())[list(char2id.values()).index(idx)] for idx in X[0]]))\n",
    "print(\"Размер словаря:\", len(char2id))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
